# global configs for experiments
batch_size: &batch_size 64
seed: 42
log_wandb: True
input_size: &input_size 224 # was 224

# data paths
dataset:
    passion:
        path: "data/PASSION"
        #path: "data/mini_data"
        split_file: "PASSION_split.csv"
    #num_workers: 24

# these keys define the eval types
dummy_uniform:
    n_folds: null
    eval_test_performance: True

dummy_constant:
    n_folds: null
    eval_test_performance: True
    constant: 1

dummy_most_frequent:
    n_folds: null
    eval_test_performance: True

fine_tuning:
    n_folds: 5
    #n_folds: 5
    eval_test_performance: True
    # method specific parameters
    train_epochs: 100
    batch_size: *batch_size
    input_size: *input_size
    learning_rate: 1.0e-05
    find_optimal_lr: True
    use_lr_scheduler: True
    warmup_epochs: 10
    early_stopping_patience: 20
    num_workers: 24
    debug: True
    # head parameters
    use_bn_in_head: True
    dropout_in_head: 0.4
    train_from_scratch: False

bias_evaluation:
    n_folds: 1
    eval_test_performance: True
    # method specific parameters
    train: False
    train_epochs: 100
    batch_size: *batch_size
    learning_rate: 1.0e-05
    find_optimal_lr: True
    use_lr_scheduler: True
    warmup_epochs: 10
    early_stopping_patience: 20
    num_workers: 24
    debug: True
    # head parameters
    use_bn_in_head: True
    dropout_in_head: 0.4

#exp1:
#    LR suggestion: steepest gradient
#    Suggested LR: 1.59E-04
#
#exp2:
#    LR suggestion: steepest gradient
#    Suggested LR: 1.48E-01
#
#exp3:
#    LR suggestion: steepest gradient
#    Suggested LR: 3.59E-04
